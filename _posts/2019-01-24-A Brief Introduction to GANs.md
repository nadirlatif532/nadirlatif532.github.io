---
published: true
layout: post
---
This post is meant to serve as a brief introduction to a subclass of Neural Nets called Generative Adversarial Nets derived from my learning of Lecture 13 of MIT's CS231 (all the pictures are from the Lecture slides). The reason I'm starting with this is because this is the latest concept I've learned and thought of sharing it with you guys. So without further ado, Let's start.  

Before diving into GANs we have to discuss the class of models it is derived from, *Generative Models which  can generate new examples from the same distribution as the training data. 
the meaning can be illustrated in a better way using this image:
![Sample vs Generated](/images/GAN_1.jpg)

As samples can be from a very complex distribution spanning N-Dimensions. It can be quite hard for a Model to learn how to generate samples from that said distribution. how GANs do this is by sample from a simple distribution such as Gaussian Noise and then transform that said distribution to the target distribution till it generates acceptable samples. 
![Generation of sample](/images/GAN_2.jpg)
__z is the random noise function from which the samples are generated.

You may be asking yourself that takes care of the Generative part what about Adversarial?

Well that where things get interested so the way GANs are trained is by splitting the problem into two halves. There's one network that Generates the sample and there is another Network that Evaluates/Discriminates the samples which are called the *Generator and *Discriminator Networks respectively. 

*Generator: Try to fool the discriminator by generating real looking samples

*Discrminator: Try to distinguish between real and fake samples.

![Fake and Real Images](/images/GAN_3.jpg)

Both networks are jointly trained where the purpose of the Generator is to keep making samples that can fool the Discriminator and the goal of the Discriminator is to correctly classify the real and fake images. So using this adversarial relationship between the two networks given a certain amount of data the generator starts to make a valid approximation of the target distribution. It is to be *noted that no explicit labels are used, the images generated by the generator are negative examples and the real one in training data are the negative examples so this effectively makes this Unsupervised Learning.

The way the two networks are in the form of a minimax game which can be using this rather complicated looking equation (which isn't that complicated):
![Mini Max](/images/GAN_4.jpg)

Let's breakdown the equation and go through it part by part:

*Dθ_d(x) = Discriminator output for real data X

*Dθ_d(Gθ_g(z) = Discriminator output for generated fake data z

*minθg: Generator (θg) wants to minimize objective such that D(G(z)) is close to 1 (discriminator is fooled into thinking generated G(z) is real)
 
*maxθd: Discriminator (θd) wants to maximize objective such that D(x) is close to 1 (real) and D(G(z)) is close to 0 (fake)

So effectively, the Generator is being trained to generate samples that can fool the discriminator and the discriminator is being trained to classify the samples correctly. How these are trained through Back-Propagation is out of the scope of this post (and my knowledge currently, I shall update once I have a better grip).

Lastly I would like to share an example implementation of GANs from Radford et al, Presented at ICLR 2016. which cleverly shows the prowess and potential of GANs
![Example Implementation](/images/GAN_5.jpg)

A lot of work is being done on GANs and it has rapidly improved over time. I urge you to take a look at ![GAN ZOO](https://github.com/hindupuravinash/the-gan-zoo "The GAN Zoo"). A list of all named GANs in a handy easily accessible repository, Thanks to ![Avinash Hindupur](https://github.com/hindupuravinash "Avinash Hindupur"). Till next time patient readers!
